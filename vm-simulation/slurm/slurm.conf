#
# NOMADE Test Cluster - SLURM Configuration
#
# This configures a single-machine SLURM installation that
# simulates a multi-node cluster with CPU and GPU partitions.
#
# Virtual Nodes:
#   cpu01, cpu02, cpu03 - CPU-only nodes (8 cores, 32GB each)
#   gpu01, gpu02        - GPU nodes (8 cores, 32GB, 1 GPU each)
#

# ============================================
# CONTROL MACHINE
# ============================================

ClusterName=nomade-test
SlurmctldHost=localhost

# ============================================
# SLURM DAEMON SETTINGS
# ============================================

SlurmUser=slurm
SlurmdUser=root
SlurmctldPort=6817
SlurmdPort=6818

# Paths
StateSaveLocation=/var/spool/slurm/ctld
SlurmdSpoolDir=/var/spool/slurm/d
SlurmctldPidFile=/var/run/slurm/slurmctld.pid
SlurmdPidFile=/var/run/slurm/slurmd.pid
SlurmctldLogFile=/var/log/slurm/slurmctld.log
SlurmdLogFile=/var/log/slurm/slurmd.log

# ============================================
# AUTHENTICATION
# ============================================

AuthType=auth/munge
CryptoType=crypto/munge

# ============================================
# PROCESS TRACKING & RESOURCE MANAGEMENT
# ============================================

ProctrackType=proctrack/cgroup
TaskPlugin=task/cgroup,task/affinity

# Job accounting
JobAcctGatherType=jobacct_gather/cgroup
JobAcctGatherFrequency=30

# ============================================
# SCHEDULING
# ============================================

SchedulerType=sched/backfill
SelectType=select/cons_tres
SelectTypeParameters=CR_Core_Memory

# Fairshare scheduling
PriorityType=priority/multifactor
PriorityDecayHalfLife=7-0
PriorityFavorSmall=NO
PriorityWeightAge=500
PriorityWeightFairshare=10000
PriorityWeightJobSize=250
PriorityWeightPartition=1000

# ============================================
# LOGGING & DEBUGGING
# ============================================

SlurmctldDebug=info
SlurmdDebug=info

# ============================================
# TIMEOUTS & LIMITS
# ============================================

InactiveLimit=0
KillWait=30
MinJobAge=300
Waittime=0

SlurmctldTimeout=120
SlurmdTimeout=300

# Default job limits
DefMemPerCPU=4000
MaxMemPerCPU=8000

# ============================================
# NODE DEFINITIONS
# ============================================
# These are virtual nodes - jobs actually run on localhost
# but SLURM tracks them as separate resources

# CPU Nodes (partition: cpu)
NodeName=cpu[01-03] \
    CPUs=8 \
    RealMemory=32000 \
    Sockets=1 \
    CoresPerSocket=8 \
    ThreadsPerCore=1 \
    State=UNKNOWN

# GPU Nodes (partition: gpu)
NodeName=gpu[01-02] \
    CPUs=8 \
    RealMemory=32000 \
    Sockets=1 \
    CoresPerSocket=8 \
    ThreadsPerCore=1 \
    Gres=gpu:1 \
    State=UNKNOWN

# ============================================
# PARTITION DEFINITIONS
# ============================================

# CPU partition (default)
PartitionName=cpu \
    Nodes=cpu[01-03] \
    Default=YES \
    MaxTime=7-00:00:00 \
    DefaultTime=01:00:00 \
    State=UP

# GPU partition
PartitionName=gpu \
    Nodes=gpu[01-02] \
    MaxTime=3-00:00:00 \
    DefaultTime=01:00:00 \
    State=UP

# Debug partition (quick jobs)
PartitionName=debug \
    Nodes=cpu01 \
    MaxTime=00:30:00 \
    DefaultTime=00:05:00 \
    State=UP

# ============================================
# GENERIC RESOURCES (GPUs)
# ============================================

GresTypes=gpu

# ============================================
# PROLOG/EPILOG
# ============================================
# Uncomment to enable NOMADE job metrics collection

#Prolog=/etc/slurm/prolog.d/*
#Epilog=/etc/slurm/epilog.d/*
#PrologFlags=Alloc

# ============================================
# ACCOUNTING
# ============================================

AccountingStorageType=accounting_storage/slurmdbd
# For simple testing without slurmdbd, use:
# AccountingStorageType=accounting_storage/filetxt
# AccountingStorageLoc=/var/log/slurm/accounting.txt

# For testing without database:
AccountingStorageType=accounting_storage/none
