# NOMADE Simulation - Large Cluster
# Production-scale cluster for stress testing and papers
#
# Node breakdown:
#   150 cpu-standard  (64 cores, 512 GB RAM, 1 TB local)
#    50 cpu-highmem   (64 cores, 1.5 TB RAM, 1 TB local)
#   100 gpu           (32 cores, 256 GB RAM, 4x A100-80GB, 2 TB local)
#
# Totals: 300 nodes | 16,000 cores | 400 GPUs | 156 TB RAM
# Shared storage: 10 PB (simulated)
#   /home:    100 TB
#   /scratch: 5 PB
#   /project: 500 TB
#   /archive: 4.4 PB

ClusterName=nomade-large
SlurmctldHost=nomade-test

# Scheduling
SchedulerType=sched/backfill
SelectType=select/cons_tres
SelectTypeParameters=CR_Core_Memory

# Logging
SlurmctldLogFile=/var/log/slurm/slurmctld.log
SlurmdLogFile=/var/log/slurm/slurmd.log
SlurmctldPidFile=/run/slurmctld.pid
SlurmdPidFile=/run/slurmd.pid

# Directories
StateSaveLocation=/var/spool/slurm/ctld
SlurmdSpoolDir=/var/spool/slurm/d

# Users
SlurmUser=slurm
SlurmdUser=root

# Timeouts
SlurmctldTimeout=300
SlurmdTimeout=300
InactiveLimit=0
MinJobAge=300
KillWait=30
Waittime=0

# Authentication
AuthType=auth/munge
CryptoType=crypto/munge

# GPU support
GresTypes=gpu

# Accounting
AccountingStorageType=accounting_storage/none
JobAcctGatherType=jobacct_gather/none

#------------------------------------------------------------------------------
# Node Definitions
#------------------------------------------------------------------------------

# CPU Standard: 64 cores, 512 GB RAM (150 nodes)
NodeName=cpu[001-150] CPUs=64 RealMemory=524288 State=UNKNOWN

# CPU High-Memory: 64 cores, 1.5 TB RAM (50 nodes)
NodeName=highmem[01-50] CPUs=64 RealMemory=1572864 State=UNKNOWN

# GPU: 32 cores, 256 GB RAM, 4x NVIDIA A100-80GB (100 nodes)
NodeName=gpu[001-100] CPUs=32 RealMemory=262144 Gres=gpu:a100:4 State=UNKNOWN

#------------------------------------------------------------------------------
# Partitions
#------------------------------------------------------------------------------

PartitionName=standard Nodes=cpu[001-150] Default=YES MaxTime=7-00:00:00 State=UP
PartitionName=highmem  Nodes=highmem[01-50] MaxTime=7-00:00:00 State=UP
PartitionName=gpu      Nodes=gpu[001-100] MaxTime=2-00:00:00 State=UP
PartitionName=debug    Nodes=cpu[001-004] MaxTime=4:00:00 DefaultTime=1:00:00 State=UP

#------------------------------------------------------------------------------
# GRES (Generic Resources) - GPU Configuration
#------------------------------------------------------------------------------
# Note: Also requires /etc/slurm/gres.conf with:
#   NodeName=gpu[001-100] Name=gpu Type=a100 File=/dev/nvidia[0-3]
